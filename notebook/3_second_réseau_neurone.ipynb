{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cb8944ac-57d5-4e2c-ac90-9994a0bacb87",
      "metadata": {
        "id": "cb8944ac-57d5-4e2c-ac90-9994a0bacb87"
      },
      "source": [
        "# Création d'un second réseau de neurone"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab781b68-1b94-4597-999f-ad6e4c4eaaf3",
      "metadata": {
        "id": "ab781b68-1b94-4597-999f-ad6e4c4eaaf3"
      },
      "source": [
        "Cette partie du code présente la façon dont nous avons construit notre second réseau de neurone, prenant en paramètre la moyenne des buts marqués par match avant que le match se joue, la moyenne de l'expected goal (xg) par match avant que le match se joue, et la moyenne des tirs cadrés par match avant que le match se joue.\n",
        "\n",
        "Le réseau de neurone suivant la méthode préconisée par François Chollet dans l'ouvrage 'L'apprentissage profond avec Python', en prenant en considération toutes les difficultés liées à l'utilisation d'un dataset avec peu de lignes (1500)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109fdd8a-b079-433f-9e9d-81d4fdb621df",
      "metadata": {
        "id": "109fdd8a-b079-433f-9e9d-81d4fdb621df"
      },
      "source": [
        "## Création du dataset pour la saison 2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5159011-6225-43aa-af80-997b9df364ab",
      "metadata": {
        "id": "c5159011-6225-43aa-af80-997b9df364ab"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger le dataset\n",
        "file_path = 'matches.csv'  # Remplacer par le chemin correct vers ton dataset\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Filtrer pour garder seulement la saison 2021\n",
        "data_2021 = data[data['season'] == 2021].copy()\n",
        "\n",
        "# Supprimer les colonnes non souhaitées\n",
        "columns_to_remove = ['Unnamed: 0', 'date', 'time', 'comp', 'day', 'attendance', 'captain', 'formation',\n",
        "                     'referee', 'match report', 'notes', 'dist', 'fk', 'pk', 'pkatt']\n",
        "data_2021.drop(columns=columns_to_remove, inplace=True)\n",
        "\n",
        "# Convertir la colonne 'round' en un numéro de semaine\n",
        "data_2021['week_number'] = data_2021['round'].apply(lambda x: int(x.split(' ')[1]))\n",
        "\n",
        "# Trier par équipe et numéro de semaine pour réinitialiser les calculs correctement\n",
        "data_2021.sort_values(by=['team', 'week_number'], inplace=True)\n",
        "\n",
        "# Initialiser les colonnes pour les cumuls et moyennes\n",
        "data_2021['som_goals'] = 0\n",
        "data_2021['moy_goals'] = 0\n",
        "data_2021['som_goals_pris'] = 0\n",
        "data_2021['moy_goals_pris'] = 0\n",
        "data_2021['som_xg'] = 0\n",
        "data_2021['moy_xg'] = 0\n",
        "data_2021['som_xga'] = 0\n",
        "data_2021['moy_xga'] = 0\n",
        "data_2021['som_poss'] = 0\n",
        "data_2021['moy_poss'] = 0\n",
        "data_2021['som_sot'] = 0\n",
        "data_2021['moy_sot'] = 0\n",
        "data_2021['total_points'] = 0\n",
        "data_2021['DB'] = 0\n",
        "\n",
        "# Calculer les valeurs pour chaque équipe indépendamment\n",
        "teams = data_2021['team'].unique()\n",
        "\n",
        "for team in teams:\n",
        "    team_data = data_2021[data_2021['team'] == team].copy()\n",
        "    team_data = team_data.sort_values(by=['week_number'])\n",
        "\n",
        "    team_data['som_goals'] = team_data['gf'].cumsum().shift().fillna(0)\n",
        "    team_data['moy_goals'] = team_data['som_goals'] / (team_data['week_number'] - 1)\n",
        "    team_data['moy_goals'] = team_data['moy_goals'].replace([float('inf'), -float('inf')], 0)\n",
        "\n",
        "    team_data['som_goals_pris'] = team_data['ga'].cumsum().shift().fillna(0)\n",
        "    team_data['moy_goals_pris'] = team_data['som_goals_pris'] / (team_data['week_number'] - 1)\n",
        "    team_data['moy_goals_pris'] = team_data['moy_goals_pris'].replace([float('inf'), -float('inf')], 0)\n",
        "\n",
        "    team_data['som_xg'] = team_data['xg'].cumsum().shift().fillna(0)\n",
        "    team_data['moy_xg'] = team_data['som_xg'] / (team_data['week_number'] - 1)\n",
        "    team_data['moy_xg'] = team_data['moy_xg'].replace([float('inf'), -float('inf')], 0)\n",
        "\n",
        "    team_data['som_xga'] = team_data['xga'].cumsum().shift().fillna(0)\n",
        "    team_data['moy_xga'] = team_data['som_xga'] / (team_data['week_number'] - 1)\n",
        "    team_data['moy_xga'] = team_data['moy_xga'].replace([float('inf'), -float('inf')], 0)\n",
        "\n",
        "    team_data['som_poss'] = team_data['poss'].cumsum().shift().fillna(0)\n",
        "    team_data['moy_poss'] = team_data['som_poss'] / (team_data['week_number'] - 1)\n",
        "    team_data['moy_poss'] = team_data['moy_poss'].replace([float('inf'), -float('inf')], 0)\n",
        "\n",
        "    team_data['som_sot'] = team_data['sot'].cumsum().shift().fillna(0)\n",
        "    team_data['moy_sot'] = team_data['som_sot'] / (team_data['week_number'] - 1)\n",
        "    team_data['moy_sot'] = team_data['moy_sot'].replace([float('inf'), -float('inf')], 0)\n",
        "\n",
        "    team_data['match_points'] = team_data['result'].map({'W': 3, 'D': 1, 'L': 0})\n",
        "    team_data['total_points'] = team_data['match_points'].cumsum().shift().fillna(0)\n",
        "\n",
        "    team_data['DB'] = team_data['som_goals'] - team_data['som_goals_pris']\n",
        "\n",
        "    data_2021.loc[team_data.index, 'som_goals'] = team_data['som_goals']\n",
        "    data_2021.loc[team_data.index, 'moy_goals'] = team_data['moy_goals']\n",
        "    data_2021.loc[team_data.index, 'som_goals_pris'] = team_data['som_goals_pris']\n",
        "    data_2021.loc[team_data.index, 'moy_goals_pris'] = team_data['moy_goals_pris']\n",
        "    data_2021.loc[team_data.index, 'som_xg'] = team_data['som_xg']\n",
        "    data_2021.loc[team_data.index, 'moy_xg'] = team_data['moy_xg']\n",
        "    data_2021.loc[team_data.index, 'som_xga'] = team_data['som_xga']\n",
        "    data_2021.loc[team_data.index, 'moy_xga'] = team_data['moy_xga']\n",
        "    data_2021.loc[team_data.index, 'som_poss'] = team_data['som_poss']\n",
        "    data_2021.loc[team_data.index, 'moy_poss'] = team_data['moy_poss']\n",
        "    data_2021.loc[team_data.index, 'som_sot'] = team_data['som_sot']\n",
        "    data_2021.loc[team_data.index, 'moy_sot'] = team_data['moy_sot']\n",
        "    data_2021.loc[team_data.index, 'total_points'] = team_data['total_points']\n",
        "    data_2021.loc[team_data.index, 'DB'] = team_data['DB']\n",
        "\n",
        "# Calculer le rang après avoir réinitialisé les valeurs\n",
        "data_2021['rank'] = 0\n",
        "unique_weeks = data_2021['week_number'].unique()\n",
        "for week in unique_weeks:\n",
        "    weekly_data = data_2021[data_2021['week_number'] == week].copy()\n",
        "    weekly_data = weekly_data.sort_values(by=['total_points', 'DB'], ascending=[False, False])\n",
        "    weekly_data['rank'] = range(1, len(weekly_data) + 1)\n",
        "    data_2021.loc[weekly_data.index, 'rank'] = weekly_data['rank']\n",
        "\n",
        "# Sauvegarder le dataset modifié\n",
        "output_file_path = 'parfait_database(1).csv'  # Remplacer par le chemin souhaité\n",
        "data_2021.to_csv(output_file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a727ac2-7675-44ce-90e3-8da5b3fd260e",
      "metadata": {
        "id": "3a727ac2-7675-44ce-90e3-8da5b3fd260e"
      },
      "source": [
        "# Création du dataset pour la saison 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c7b42a-a61b-4870-aea8-447c62cac0e8",
      "metadata": {
        "id": "98c7b42a-a61b-4870-aea8-447c62cac0e8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger le dataset\n",
        "file_path = 'matches.csv'  # Remplacer par le chemin correct vers ton dataset\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Filtrer pour garder seulement la saison 2021\n",
        "data_2021 = data[data['season'] == 2022].copy()\n",
        "\n",
        "# Supprimer les colonnes non souhaitées\n",
        "columns_to_remove = ['Unnamed: 0', 'date', 'time', 'comp', 'day', 'attendance', 'captain', 'formation',\n",
        "                     'referee', 'match report', 'notes', 'dist', 'fk', 'pk', 'pkatt']\n",
        "data_2021.drop(columns=columns_to_remove, inplace=True)\n",
        "\n",
        "# Convertir la colonne 'round' en un numéro de semaine\n",
        "data_2021['week_number'] = data_2021['round'].apply(lambda x: int(x.split(' ')[1]))\n",
        "\n",
        "# Trier par équipe et numéro de semaine pour réinitialiser les calculs correctement\n",
        "data_2021.sort_values(by=['team', 'week_number'], inplace=True)\n",
        "\n",
        "# Initialiser les colonnes pour les cumuls et moyennes\n",
        "data_2021['som_goals'] = 0\n",
        "data_2021['moy_goals'] = 0\n",
        "data_2021['som_goals_pris'] = 0\n",
        "data_2021['moy_goals_pris'] = 0\n",
        "data_2021['som_xg'] = 0\n",
        "data_2021['moy_xg'] = 0\n",
        "data_2021['som_xga'] = 0\n",
        "data_2021['moy_xga'] = 0\n",
        "data_2021['som_poss'] = 0\n",
        "data_2021['moy_poss'] = 0\n",
        "data_2021['som_sot'] = 0\n",
        "data_2021['moy_sot'] = 0\n",
        "data_2021['total_points'] = 0\n",
        "data_2021['DB'] = 0\n",
        "\n",
        "# Calculer les valeurs pour chaque équipe indépendamment\n",
        "teams = data_2021['team'].unique()\n",
        "\n",
        "for team in teams:\n",
        "    team_data = data_2021[data_2021['team'] == team].copy()\n",
        "    team_data = team_data.sort_values(by=['week_number'])\n",
        "\n",
        "    team_data['som_goals'] = team_data['gf'].cumsum().shift().fillna(0)\n",
        "    team_data['moy_goals'] = team_data['som_goals'] / (team_data['week_number'] - 1)\n",
        "    team_data['moy_goals'] = team_data['moy_goals'].replace([float('inf'), -float('inf')], 0)\n",
        "\n",
        "    team_data['som_goals_pris'] = team_data['ga'].cumsum().shift().fillna(0)\n",
        "    team_data['moy_goals_pris'] = team_data['som_goals_pris'] / (team_data['week_number'] - 1)\n",
        "    team_data['moy_goals_pris'] = team_data['moy_goals_pris'].replace([float('inf'), -float('inf')], 0)\n",
        "\n",
        "    team_data['som_xg'] = team_data['xg'].cumsum().shift().fillna(0)\n",
        "    team_data['moy_xg'] = team_data['som_xg'] / (team_data['week_number'] - 1)\n",
        "    team_data['moy_xg'] = team_data['moy_xg'].replace([float('inf'), -float('inf')], 0)\n",
        "\n",
        "    team_data['som_xga'] = team_data['xga'].cumsum().shift().fillna(0)\n",
        "    team_data['moy_xga'] = team_data['som_xga'] / (team_data['week_number'] - 1)\n",
        "    team_data['moy_xga'] = team_data['moy_xga'].replace([float('inf'), -float('inf')], 0)\n",
        "\n",
        "    team_data['som_poss'] = team_data['poss'].cumsum().shift().fillna(0)\n",
        "    team_data['moy_poss'] = team_data['som_poss'] / (team_data['week_number'] - 1)\n",
        "    team_data['moy_poss'] = team_data['moy_poss'].replace([float('inf'), -float('inf')], 0)\n",
        "\n",
        "    team_data['som_sot'] = team_data['sot'].cumsum().shift().fillna(0)\n",
        "    team_data['moy_sot'] = team_data['som_sot'] / (team_data['week_number'] - 1)\n",
        "    team_data['moy_sot'] = team_data['moy_sot'].replace([float('inf'), -float('inf')], 0)\n",
        "\n",
        "    team_data['match_points'] = team_data['result'].map({'W': 3, 'D': 1, 'L': 0})\n",
        "    team_data['total_points'] = team_data['match_points'].cumsum().shift().fillna(0)\n",
        "\n",
        "    team_data['DB'] = team_data['som_goals'] - team_data['som_goals_pris']\n",
        "\n",
        "    data_2021.loc[team_data.index, 'som_goals'] = team_data['som_goals']\n",
        "    data_2021.loc[team_data.index, 'moy_goals'] = team_data['moy_goals']\n",
        "    data_2021.loc[team_data.index, 'som_goals_pris'] = team_data['som_goals_pris']\n",
        "    data_2021.loc[team_data.index, 'moy_goals_pris'] = team_data['moy_goals_pris']\n",
        "    data_2021.loc[team_data.index, 'som_xg'] = team_data['som_xg']\n",
        "    data_2021.loc[team_data.index, 'moy_xg'] = team_data['moy_xg']\n",
        "    data_2021.loc[team_data.index, 'som_xga'] = team_data['som_xga']\n",
        "    data_2021.loc[team_data.index, 'moy_xga'] = team_data['moy_xga']\n",
        "    data_2021.loc[team_data.index, 'som_poss'] = team_data['som_poss']\n",
        "    data_2021.loc[team_data.index, 'moy_poss'] = team_data['moy_poss']\n",
        "    data_2021.loc[team_data.index, 'som_sot'] = team_data['som_sot']\n",
        "    data_2021.loc[team_data.index, 'moy_sot'] = team_data['moy_sot']\n",
        "    data_2021.loc[team_data.index, 'total_points'] = team_data['total_points']\n",
        "    data_2021.loc[team_data.index, 'DB'] = team_data['DB']\n",
        "\n",
        "# Calculer le rang après avoir réinitialisé les valeurs\n",
        "data_2021['rank'] = 0\n",
        "unique_weeks = data_2021['week_number'].unique()\n",
        "for week in unique_weeks:\n",
        "    weekly_data = data_2021[data_2021['week_number'] == week].copy()\n",
        "    weekly_data = weekly_data.sort_values(by=['total_points', 'DB'], ascending=[False, False])\n",
        "    weekly_data['rank'] = range(1, len(weekly_data) + 1)\n",
        "    data_2021.loc[weekly_data.index, 'rank'] = weekly_data['rank']\n",
        "\n",
        "# Sauvegarder le dataset modifié\n",
        "output_file_path = 'parfait_database(2).csv'  # Remplacer par le chemin souhaité\n",
        "data_2021.to_csv(output_file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e40670-7c77-4f6d-8753-b354a68b3580",
      "metadata": {
        "id": "07e40670-7c77-4f6d-8753-b354a68b3580"
      },
      "source": [
        "# Concaténation des deux datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c90b42ac-231d-4a4c-b0e0-9ccc37216c75",
      "metadata": {
        "id": "c90b42ac-231d-4a4c-b0e0-9ccc37216c75"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger les deux datasets\n",
        "dataset_2021_path = 'parfait_database(1).csv'\n",
        "dataset_2022_path = 'parfait_database(2).csv'\n",
        "\n",
        "dataset_2021 = pd.read_csv(dataset_2021_path)\n",
        "dataset_2022 = pd.read_csv(dataset_2022_path)\n",
        "\n",
        "# Concatenation des deux datasets\n",
        "combined_dataset = pd.concat([dataset_2021, dataset_2022], ignore_index=True)\n",
        "\n",
        "# Sauvegarder le dataset combiné\n",
        "output_combined_path = 'combined_dataset_2021_2022.csv'\n",
        "combined_dataset.to_csv(output_combined_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49d0bad-891f-4ca3-9631-ab182585ecda",
      "metadata": {
        "id": "e49d0bad-891f-4ca3-9631-ab182585ecda"
      },
      "source": [
        "# 1) Premier code pour obtenir un résultat en se basant sur le cours et la méthode d'optimisation ADAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c349a25c-f1ad-455e-99f2-3c94d359dc52",
      "metadata": {
        "id": "c349a25c-f1ad-455e-99f2-3c94d359dc52"
      },
      "outputs": [],
      "source": [
        "#Code cours\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Prétraitement des données\n",
        "data['venue'] = data['venue'].apply(lambda x: 1 if x == 'Home' else 0)\n",
        "data.fillna(0, inplace=True)  # Remplacer les NaN par 0 pour la première journée\n",
        "\n",
        "# Sélectionner les colonnes pertinentes\n",
        "features = ['venue','moy_goals', 'moy_xg', 'moy_sot']\n",
        "target = 'result'\n",
        "\n",
        "# Encoder le résultat\n",
        "label_encoder = LabelEncoder()\n",
        "data['result_encoded'] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "# Séparer les features et le target\n",
        "X = data[features]\n",
        "y = data['result_encoded']\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Construire le modèle\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(3, activation='softmax')  # 3 classes pour W, D, L\n",
        "])\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entraîner le modèle\n",
        "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Évaluer le modèle\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a180b3a-942d-4246-8b4f-ee08f84f377f",
      "metadata": {
        "id": "5a180b3a-942d-4246-8b4f-ee08f84f377f"
      },
      "source": [
        "## 1.1 Nettoyage de données : Code cours actualisé avec la première ligne qu'on ne prend pas en compte car ce n'est pas representatif"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9806b45-e59e-4bf4-895c-7d3bc7c1f2a8",
      "metadata": {
        "id": "b9806b45-e59e-4bf4-895c-7d3bc7c1f2a8"
      },
      "source": [
        "## 1.2 On a normalisé les données à l'aide de StandarScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec1dd3f-4f0c-4419-9638-3d4a9073c494",
      "metadata": {
        "id": "fec1dd3f-4f0c-4419-9638-3d4a9073c494"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Exclure les matchs de la première semaine\n",
        "data = data[data['round'] != 'Matchweek 1']\n",
        "\n",
        "# Prétraitement des données\n",
        "data['venue'] = data['venue'].apply(lambda x: 1 if x == 'Home' else 0)\n",
        "\n",
        "# Sélectionner les colonnes pertinentes\n",
        "features = ['venue','moy_goals', 'moy_xg', 'moy_sot']\n",
        "target = 'result'\n",
        "\n",
        "# Encoder le résultat\n",
        "label_encoder = LabelEncoder()\n",
        "data['result_encoded'] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "# Séparer les features et le target\n",
        "X = data[features]\n",
        "y = data['result_encoded']\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Construire le modèle\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(3, activation='softmax')  # 3 classes pour W, D, L\n",
        "])\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entraîner le modèle\n",
        "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Évaluer le modèle\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test accuracy: {accuracy * 100:.2f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa487862-a33d-4bf3-a218-3195277dd5f6",
      "metadata": {
        "id": "fa487862-a33d-4bf3-a218-3195277dd5f6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Exclure les matchs de la première semaine\n",
        "data = data[data['round'] != 'Matchweek 1']\n",
        "\n",
        "# Prétraitement des données\n",
        "data['venue'] = data['venue'].apply(lambda x: 1 if x == 'Home' else 0)\n",
        "\n",
        "# Sélectionner les colonnes pertinentes\n",
        "features = ['venue', 'moy_goals', 'moy_xg', 'moy_sot']\n",
        "target = 'result'\n",
        "\n",
        "# Encoder le résultat\n",
        "label_encoder = LabelEncoder()\n",
        "data['result_encoded'] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "# Séparer les features et le target\n",
        "X = data[features]\n",
        "y = data['result_encoded']\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Construire le modèle\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(3, activation='softmax')  # 3 classes pour W, D, L\n",
        "])\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entraîner le modèle\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Évaluer le modèle\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Tracer les courbes de perte et d'exactitude\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63737393-2684-4516-8621-281931302cf2",
      "metadata": {
        "id": "63737393-2684-4516-8621-281931302cf2"
      },
      "source": [
        "Ces tracès sont caractéristiques d'un sur-ajustement. L'exactitude de l'entrainement augmente linéaire dans le temps, alors que l'exactitude de la validation se stabilise autour de 95%.\n",
        "La perte de validation atteint son minimum après 40 Epoch (semblant de bruit), puis décroche que la perte d'entrainement continue à diminuer linéairement."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "078676eb-b618-45f2-a350-a48d3eea0bef",
      "metadata": {
        "id": "078676eb-b618-45f2-a350-a48d3eea0bef"
      },
      "source": [
        "## 1.3 Augmentation des données par suréchantillonage avec la méthode SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5a1253a-bcbf-42f2-8345-4cf939861115",
      "metadata": {
        "id": "a5a1253a-bcbf-42f2-8345-4cf939861115"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Exclure les matchs de la première semaine\n",
        "data = data[data['round'] != 'Matchweek 1']\n",
        "\n",
        "# Prétraitement des données\n",
        "data['venue'] = data['venue'].apply(lambda x: 1 if x == 'Home' else 0)\n",
        "\n",
        "# Sélectionner les colonnes pertinentes\n",
        "features = ['venue', 'moy_goals', 'moy_xg', 'moy_sot']\n",
        "target = 'result'\n",
        "\n",
        "# Encoder le résultat\n",
        "label_encoder = LabelEncoder()\n",
        "data['result_encoded'] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "# Séparer les features et le target\n",
        "X = data[features]\n",
        "y = data['result_encoded']\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Suréchantillonnage avec SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Construire le modèle\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(3, activation='softmax')  # 3 classes pour W, D, L\n",
        "])\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entraîner le modèle\n",
        "model.fit(X_train_scaled, y_train_smote, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Évaluer le modèle\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0971cae9-5e22-4d3a-9302-490625a23dfe",
      "metadata": {
        "id": "0971cae9-5e22-4d3a-9302-490625a23dfe"
      },
      "source": [
        "# 2) Architecture du réseau :\n",
        "## 2.1 Choix de l'architecture :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64133ac5-173e-4b79-8b60-f41a4c9ac192",
      "metadata": {
        "id": "64133ac5-173e-4b79-8b60-f41a4c9ac192"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Exclure les matchs de la première semaine\n",
        "data = data[data['round'] != 'Matchweek 1']\n",
        "\n",
        "# Prétraitement des données\n",
        "data['venue'] = data['venue'].apply(lambda x: 1 if x == 'Home' else 0)\n",
        "\n",
        "# Sélectionner les colonnes pertinentes\n",
        "features = ['venue', 'moy_goals', 'moy_xg', 'moy_sot']\n",
        "target = 'result'\n",
        "\n",
        "# Encoder le résultat\n",
        "label_encoder = LabelEncoder()\n",
        "data['result_encoded'] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "# Séparer les features et le target\n",
        "X = data[features]\n",
        "y = data['result_encoded']\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Suréchantillonnage avec SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "def build_model(architecture_type):\n",
        "    model = Sequential()\n",
        "    if architecture_type == 'deeper':\n",
        "        model.add(Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "    elif architecture_type == 'wider':\n",
        "        model.add(Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "    elif architecture_type == 'deep_and_regularized':\n",
        "        model.add(Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "\n",
        "    model.add(Dense(3, activation='softmax'))  # 3 classes pour W, D, L\n",
        "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Tester différentes architectures\n",
        "for architecture in ['deeper', 'wider', 'deep_and_regularized']:\n",
        "    model = build_model(architecture)\n",
        "    print(f\"Training model: {architecture}\")\n",
        "    model.fit(X_train_scaled, y_train_smote, epochs=50, batch_size=32, validation_split=0.2)\n",
        "    loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "    print(f\"Test accuracy for {architecture}: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "565bc383-81e1-4fe2-99c8-3b4dd680ee7a",
      "metadata": {
        "id": "565bc383-81e1-4fe2-99c8-3b4dd680ee7a"
      },
      "source": [
        "On compare différentes architectures réseaux : une plus large, une plus profonde et une profonde et régularisée. Il s'avère que c'est cette dernière qui permet d'obtenir la meilleur accuracy, à savoir 48,52%"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b9363d-dfa9-46b1-b39c-39ff668b090a",
      "metadata": {
        "id": "33b9363d-dfa9-46b1-b39c-39ff668b090a"
      },
      "source": [
        "## 2.2 Choix des hyperparamètres"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c798b4-d5b0-421a-ad59-2853164e8e4a",
      "metadata": {
        "id": "e2c798b4-d5b0-421a-ad59-2853164e8e4a"
      },
      "source": [
        "Maitenant que l'on sait que quel architecture réseau est la meilleure pour notre modèle, on se propose de choisir de manière optimale les hyperparamètres de cette architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f296394-5570-4ca0-b559-ffc29229ecb2",
      "metadata": {
        "id": "6f296394-5570-4ca0-b559-ffc29229ecb2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Exclure les matchs de la première semaine\n",
        "data = data[data['round'] != 'Matchweek 1']\n",
        "\n",
        "# Prétraitement des données\n",
        "data['venue'] = data['venue'].apply(lambda x: 1 if x == 'Home' else 0)\n",
        "\n",
        "# Sélectionner les colonnes pertinentes\n",
        "features = ['venue', 'moy_goals', 'moy_xg', 'moy_sot']\n",
        "target = 'result'\n",
        "\n",
        "# Encoder le résultat\n",
        "label_encoder = LabelEncoder()\n",
        "data['result_encoded'] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "# Séparer les features et le target\n",
        "X = data[features]\n",
        "y = data['result_encoded']\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Suréchantillonnage avec SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Définir les hyperparamètres à tester\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "batch_sizes = [16, 32, 64]\n",
        "num_epochs = [30, 50, 70]\n",
        "\n",
        "def build_model(lr):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "        Dropout(0.2),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(3, activation='softmax')  # 3 classes pour W, D, L\n",
        "    ])\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Tester différentes combinaisons de hyperparamètres\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "        for epochs in num_epochs:\n",
        "            print(f\"Training model with LR={lr}, batch_size={batch_size}, epochs={epochs}\")\n",
        "            model = build_model(lr)\n",
        "            model.fit(X_train_scaled, y_train_smote, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
        "            loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "            print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = {'learning_rate': lr, 'batch_size': batch_size, 'epochs': epochs}\n",
        "\n",
        "print(f\"Best accuracy: {best_accuracy * 100:.2f}% with parameters {best_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9c278db-9f34-4ed7-88bf-9c517afe5bdc",
      "metadata": {
        "id": "c9c278db-9f34-4ed7-88bf-9c517afe5bdc"
      },
      "source": [
        "Le code ci-dessus nous permet d'obtenir les meilleurs hyperparamètres, et avec une accuracy de 53%, ce qui est une accuracy très proche de celle obtenue seulement avec les côtes, c'est pourquoi on se propose par la suite d'améliorer encore le code avec des techniques de régularisation et de validation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60fabb91-cbb6-4dd6-a904-94102dbdfb60",
      "metadata": {
        "id": "60fabb91-cbb6-4dd6-a904-94102dbdfb60"
      },
      "source": [
        "# 3) Régularisation L2\n",
        "La régularisation permet de rendre le modèle plus résistant au sur-ajustement. Vérifions si notre modèle est sur-ajusté.\n",
        "Pour vérifier si le sur-ajustement est présent, les performances des modèles sur des données nouvelles (never-before-seen-data) ont commencé à ralentir, contrairement à leurs performances sur les données de l'ensemble d'entrainement, performances qui s'améliorent toujours au fur et à mesure que l'entrainement progresse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd724e2f-a962-4202-aede-a6320aa9463e",
      "metadata": {
        "id": "dd724e2f-a962-4202-aede-a6320aa9463e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Exclure les matchs de la première semaine\n",
        "data = data[data['round'] != 'Matchweek 1']\n",
        "\n",
        "# Prétraitement des données\n",
        "data['venue'] = data['venue'].apply(lambda x: 1 if x == 'Home' else 0)\n",
        "\n",
        "# Sélectionner les colonnes pertinentes\n",
        "features = ['venue', 'moy_goals', 'moy_xg', 'moy_sot']\n",
        "target = 'result'\n",
        "\n",
        "# Encoder le résultat\n",
        "label_encoder = LabelEncoder()\n",
        "data['result_encoded'] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "# Séparer les features et le target\n",
        "X = data[features]\n",
        "y = data['result_encoded']\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Suréchantillonnage avec SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Définir les hyperparamètres optimaux\n",
        "best_lr = 0.01\n",
        "best_batch_size = 16\n",
        "best_epochs = 30\n",
        "\n",
        "# Construire le modèle avec régularisation L2\n",
        "def build_model():\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],), kernel_regularizer=l2(0.0001)),\n",
        "        Dropout(0.2),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.0001)),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu', kernel_regularizer=l2(0.0001)),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l2(0.0001)),\n",
        "        Dense(3, activation='softmax')  # 3 classes pour W, D, L\n",
        "    ])\n",
        "    optimizer = Adam(learning_rate=best_lr)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Créer et entraîner le modèle\n",
        "model = build_model()\n",
        "model.fit(X_train_scaled, y_train_smote, epochs=best_epochs, batch_size=best_batch_size, validation_split=0.2)\n",
        "\n",
        "# Évaluer le modèle\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0ab2ed0-75aa-477f-b728-34ba331cfd6c",
      "metadata": {
        "id": "c0ab2ed0-75aa-477f-b728-34ba331cfd6c"
      },
      "source": [
        "On obtient une accuracy de 52,59%, donc la technique de régularisation semble confirmer cette approche par les bookmaker d'utiliser ces principales caractéristiques pour prédire le score d'un match"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f3fa187-8864-4a2d-a63b-d3a2f629a50e",
      "metadata": {
        "id": "9f3fa187-8864-4a2d-a63b-d3a2f629a50e"
      },
      "source": [
        "# 4) Techniques de Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35018f91-d0d5-4eb9-b59a-493c990cf87b",
      "metadata": {
        "id": "35018f91-d0d5-4eb9-b59a-493c990cf87b"
      },
      "source": [
        "On va réaliser une première technique de validation croisée K-fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bf154e0-ecd0-46ba-bd48-5a438e655147",
      "metadata": {
        "id": "0bf154e0-ecd0-46ba-bd48-5a438e655147"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import numpy as np\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Exclure les matchs de la première semaine et réinitialiser l'index\n",
        "data = data[data['round'] != 'Matchweek 1'].reset_index(drop=True)\n",
        "\n",
        "# Prétraitement des données\n",
        "data['venue'] = data['venue'].apply(lambda x: 1 if x == 'Home' else 0)\n",
        "\n",
        "# Sélectionner les colonnes pertinentes\n",
        "features = ['venue', 'moy_goals', 'moy_xg', 'moy_sot']\n",
        "target = 'result'\n",
        "\n",
        "# Encoder le résultat\n",
        "label_encoder = LabelEncoder()\n",
        "data['result_encoded'] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "# Séparer les features et le target\n",
        "X = data[features].values\n",
        "y = data['result_encoded'].values\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Définir les hyperparamètres optimaux\n",
        "best_lr = 0.01\n",
        "best_batch_size = 16\n",
        "best_epochs = 30\n",
        "\n",
        "# Définir la régularisation L2\n",
        "reg = l2(0.001)\n",
        "\n",
        "# Définir la validation croisée K-Fold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "accuracies = []\n",
        "\n",
        "for train_index, test_index in kf.split(X_scaled, y):\n",
        "    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "    # Construire le modèle\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', kernel_regularizer=reg),\n",
        "        Dropout(0.2),\n",
        "        Dense(128, activation='relu', kernel_regularizer=reg),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu', kernel_regularizer=reg),\n",
        "        Dense(32, activation='relu', kernel_regularizer=reg),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compiler le modèle\n",
        "    optimizer = Adam(learning_rate=best_lr)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Entraîner le modèle\n",
        "    print(f'Training for fold {fold_no}...')\n",
        "    model.fit(X_train_fold, y_train_fold, batch_size=best_batch_size, epochs=best_epochs, verbose=1)\n",
        "\n",
        "    # Évaluer le modèle\n",
        "    scores = model.evaluate(X_test_fold, y_test_fold, verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    accuracies.append(scores[1] * 100)\n",
        "    fold_no += 1\n",
        "\n",
        "print(f'Average accuracy over all folds: {np.mean(accuracies)}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18b560b9-b13a-4e89-83fe-9b25391a9a5a",
      "metadata": {
        "id": "18b560b9-b13a-4e89-83fe-9b25391a9a5a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Exclure les matchs de la première semaine et réinitialiser l'index\n",
        "data = data[data['round'] != 'Matchweek 1'].reset_index(drop=True)\n",
        "\n",
        "# Prétraitement des données\n",
        "data['venue'] = data['venue'].apply(lambda x: 1 if x == 'Home' else 0)\n",
        "\n",
        "# Sélectionner les colonnes pertinentes\n",
        "features = ['venue', 'moy_goals', 'moy_xg', 'moy_sot']\n",
        "target = 'result'\n",
        "\n",
        "# Encoder le résultat\n",
        "label_encoder = LabelEncoder()\n",
        "data['result_encoded'] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "# Séparer les features et le target\n",
        "X = data[features].values\n",
        "y = data['result_encoded'].values\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Définir les hyperparamètres optimaux\n",
        "best_lr = 0.01\n",
        "best_batch_size = 16\n",
        "best_epochs = 30\n",
        "\n",
        "# Définir la régularisation L2\n",
        "reg = l2(0.001)\n",
        "\n",
        "# Définir la validation croisée K-Fold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "accuracies = []\n",
        "\n",
        "for train_index, test_index in kf.split(X_scaled, y):\n",
        "    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "    # Créer un ensemble de validation à partir de l'ensemble d'entraînement\n",
        "    val_split_index = int(0.9 * len(X_train_fold))  # 10% pour la validation\n",
        "    X_train, X_val = X_train_fold[:val_split_index], X_train_fold[val_split_index:]\n",
        "    y_train, y_val = y_train_fold[:val_split_index], y_train_fold[val_split_index:]\n",
        "\n",
        "    # Construire le modèle\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', kernel_regularizer=reg),\n",
        "        Dropout(0.2),\n",
        "        Dense(128, activation='relu', kernel_regularizer=reg),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu', kernel_regularizer=reg),\n",
        "        Dense(32, activation='relu', kernel_regularizer=reg),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compiler le modèle\n",
        "    optimizer = Adam(learning_rate=best_lr)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Entraîner le modèle avec l'ensemble de validation\n",
        "    print(f'Training for fold {fold_no}...')\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                        batch_size=best_batch_size, epochs=best_epochs, verbose=1)\n",
        "\n",
        "    # Évaluer le modèle\n",
        "    scores = model.evaluate(X_test_fold, y_test_fold, verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    accuracies.append(scores[1] * 100)\n",
        "\n",
        "    # Tracer les courbes de perte et d'exactitude\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'Loss for Fold {fold_no}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'Accuracy for Fold {fold_no}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "print(f'Average accuracy over all folds: {np.mean(accuracies)}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4976fea3-8553-4e06-bdaa-8bce9d94f84f",
      "metadata": {
        "id": "4976fea3-8553-4e06-bdaa-8bce9d94f84f"
      },
      "source": [
        "En utilisant de technique de régularisation encore plus poussée et en ajustant les hyperparamètres du réseau, on obtient une exactitude de prédiction encore meilleure, aux alentours de 48%. Mais il serait difficile d'améliorer ces perfomances en partant de si peu de données. Afin d'améliorer notre exactitude de prédiction sur ce problème, l'étape suivante serait d'utiliser un modèle de machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f4a123b-7045-484f-84ce-6cc90cc9fdc6",
      "metadata": {
        "id": "6f4a123b-7045-484f-84ce-6cc90cc9fdc6"
      },
      "source": [
        "La technique de validation semble montrer une baisse d'accuracy, ici de 47,2%, on tend vers un modèle plus proche de la réalite. Réalisons cette même technique de validation en intégrant SMOTE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a59d3883-cfab-4e99-9b01-103507f7d8bf",
      "metadata": {
        "id": "a59d3883-cfab-4e99-9b01-103507f7d8bf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Exclure les matchs de la première semaine et réinitialiser l'index\n",
        "data = data[data['round'] != 'Matchweek 1'].reset_index(drop=True)\n",
        "\n",
        "# Prétraitement des données\n",
        "data['venue'] = data['venue'].apply(lambda x: 1 if x == 'Home' else 0)\n",
        "\n",
        "# Sélectionner les colonnes pertinentes\n",
        "features = ['venue', 'moy_goals', 'moy_xg', 'moy_sot']\n",
        "target = 'result'\n",
        "\n",
        "# Encoder le résultat\n",
        "label_encoder = LabelEncoder()\n",
        "data['result_encoded'] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "# Séparer les features et le target\n",
        "X = data[features].values\n",
        "y = data['result_encoded'].values\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Définir les hyperparamètres optimaux\n",
        "best_lr = 0.01\n",
        "best_batch_size = 16\n",
        "best_epochs = 30\n",
        "\n",
        "# Définir la régularisation L2\n",
        "reg = l2(0.001)\n",
        "\n",
        "# Définir la validation croisée K-Fold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "accuracies = []\n",
        "\n",
        "for train_index, test_index in kf.split(X_scaled, y):\n",
        "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Diviser les données d'entraînement pour obtenir un ensemble de validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Suréchantillonnage avec SMOTE pour les données d'entraînement\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Construire le modèle\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', kernel_regularizer=reg),\n",
        "        Dropout(0.2),\n",
        "        Dense(128, activation='relu', kernel_regularizer=reg),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu', kernel_regularizer=reg),\n",
        "        Dense(32, activation='relu', kernel_regularizer=reg),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compiler le modèle\n",
        "    optimizer = Adam(learning_rate=best_lr)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Entraîner le modèle avec la validation\n",
        "    model.fit(X_train_smote, y_train_smote, batch_size=best_batch_size, epochs=best_epochs, verbose=1, validation_data=(X_val, y_val))\n",
        "\n",
        "    # Évaluer le modèle sur l'ensemble de test\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    accuracies.append(scores[1] * 100)\n",
        "\n",
        "print(f'Average accuracy over all folds: {np.mean(accuracies)}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb3cf4a-397c-49ee-8e58-a07a63b58595",
      "metadata": {
        "id": "dfb3cf4a-397c-49ee-8e58-a07a63b58595"
      },
      "source": [
        "Réalisons une dernière technique de validation, la validation croisée K-Fold itérative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e465e0d-48a2-4b98-94a6-86f5617232e9",
      "metadata": {
        "id": "6e465e0d-48a2-4b98-94a6-86f5617232e9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import numpy as np\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Exclure les matchs de la première semaine et réinitialiser l'index\n",
        "data = data[data['round'] != 'Matchweek 1'].reset_index(drop=True)\n",
        "\n",
        "# Prétraitement des données\n",
        "data['venue'] = data['venue'].apply(lambda x: 1 if x == 'Home' else 0)\n",
        "features = ['venue', 'moy_goals', 'moy_xg', 'moy_sot']\n",
        "target = 'result'\n",
        "\n",
        "# Encoder le résultat\n",
        "label_encoder = LabelEncoder()\n",
        "data['result_encoded'] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "# Séparer les features et le target\n",
        "X = data[features].values\n",
        "y = data['result_encoded'].values\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Paramètres de la validation croisée\n",
        "n_splits = 5\n",
        "n_repeats = 10\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Paramètres du modèle\n",
        "best_lr = 0.01\n",
        "best_batch_size = 16\n",
        "best_epochs = 30\n",
        "reg = l2(0.001)\n",
        "\n",
        "# Boucle pour la validation croisée itérée\n",
        "overall_scores = []\n",
        "for i in range(n_repeats):\n",
        "    fold_scores = []\n",
        "    for train_index, test_index in kf.split(X_scaled):\n",
        "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # Construire le modèle\n",
        "        model = Sequential([\n",
        "            Dense(128, activation='relu', kernel_regularizer=reg),\n",
        "            Dropout(0.2),\n",
        "            Dense(128, activation='relu', kernel_regularizer=reg),\n",
        "            Dropout(0.2),\n",
        "            Dense(64, activation='relu', kernel_regularizer=reg),\n",
        "            Dense(32, activation='relu', kernel_regularizer=reg),\n",
        "            Dense(3, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        # Compiler le modèle\n",
        "        optimizer = Adam(learning_rate=best_lr)\n",
        "        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Entraîner le modèle\n",
        "        model.fit(X_train, y_train, batch_size=best_batch_size, epochs=best_epochs, verbose=0)\n",
        "\n",
        "        # Évaluer le modèle\n",
        "        scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "        fold_scores.append(scores[1] * 100)\n",
        "\n",
        "    overall_scores.append(np.mean(fold_scores))\n",
        "    print(f\"Iteration {i+1}/{n_repeats}, Mean Accuracy: {np.mean(fold_scores):.2f}%\")\n",
        "\n",
        "print(f'Final Mean Accuracy over all iterations: {np.mean(overall_scores):.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7711be5b-4e54-4258-ad1c-daed82c84b3d",
      "metadata": {
        "id": "7711be5b-4e54-4258-ad1c-daed82c84b3d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Exclure les matchs de la première semaine et réinitialiser l'index\n",
        "data = data[data['round'] != 'Matchweek 1'].reset_index(drop=True)\n",
        "\n",
        "# Prétraitement des données\n",
        "data['venue'] = data['venue'].apply(lambda x: 1 if x == 'Home' else 0)\n",
        "features = ['venue', 'moy_goals', 'moy_xg', 'moy_sot']\n",
        "target = 'result'\n",
        "\n",
        "# Encoder le résultat\n",
        "label_encoder = LabelEncoder()\n",
        "data['result_encoded'] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "# Séparer les features et le target\n",
        "X = data[features].values\n",
        "y = data['result_encoded'].values\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Paramètres de la validation croisée itérative\n",
        "n_splits = 5\n",
        "n_repeats = 10\n",
        "rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
        "\n",
        "# Paramètres du modèle\n",
        "best_lr = 0.01\n",
        "best_batch_size = 16\n",
        "best_epochs = 30\n",
        "reg = l2(0.001)\n",
        "\n",
        "# Boucle pour la validation croisée itérée\n",
        "overall_scores = []\n",
        "fold_histories = []\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(rkf.split(X_scaled)):\n",
        "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Construire le modèle\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', kernel_regularizer=reg),\n",
        "        Dropout(0.2),\n",
        "        Dense(128, activation='relu', kernel_regularizer=reg),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu', kernel_regularizer=reg),\n",
        "        Dense(32, activation='relu', kernel_regularizer=reg),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compiler le modèle\n",
        "    optimizer = Adam(learning_rate=best_lr)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Entraîner le modèle\n",
        "    history = model.fit(X_train, y_train, batch_size=best_batch_size, epochs=best_epochs, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "    # Stocker l'historique pour les tracés\n",
        "    fold_histories.append(history.history)\n",
        "\n",
        "    # Évaluer le modèle\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    overall_scores.append(scores[1] * 100)\n",
        "\n",
        "    # Tracer les courbes après chaque fold\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'Loss for Fold {i+1}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'Accuracy for Fold {i+1}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print(f'Final Mean Accuracy over all iterations: {np.mean(overall_scores):.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "172bbcd0-3c93-4c21-a5bf-f534822d05d6",
      "metadata": {
        "id": "172bbcd0-3c93-4c21-a5bf-f534822d05d6"
      },
      "source": [
        "# MATRICE DE CORRELATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5dbaeab-37a0-4ed9-b381-4be620750707",
      "metadata": {
        "id": "f5dbaeab-37a0-4ed9-b381-4be620750707"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Charger les données\n",
        "data = pd.read_csv('combined_dataset_2021_2022.csv')\n",
        "\n",
        "# Transformer la colonne 'result' en valeurs numériques\n",
        "data['result'] = data['result'].map({'W': 1, 'D': 0, 'L': -1})\n",
        "\n",
        "# Supprimer les colonnes contenant du texte\n",
        "data = data.select_dtypes(include=[np.number])  # Garde uniquement les colonnes numériques\n",
        "\n",
        "# Calculer la matrice de corrélation\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Visualiser la matrice de corrélation\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Matrice de Corrélation')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c1721f-545a-49c2-af8b-5eb02f288ef1",
      "metadata": {
        "id": "15c1721f-545a-49c2-af8b-5eb02f288ef1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
